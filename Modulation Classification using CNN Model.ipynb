{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Paper is directly implemented from: \n",
    "#T. J. O’Shea, J. Corgan, and T. C. Clancy, “Convolutional radio modulation recognition networks,” in International\n",
    "#conference on engineering applications of neural networks. Springer, 2016, pp. 213–226.\n",
    "\n",
    "import tarfile\n",
    "import pickle\n",
    "import random\n",
    "import gc\n",
    "\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import scipy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame as df\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils, plot_model\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D, SeparableConv2D\n",
    "from keras.layers import UpSampling2D, GlobalAveragePooling2D, AveragePooling2D, BatchNormalization\n",
    "from keras.layers import Input, LSTM, Dense, Concatenate,add\n",
    "from keras.layers.core import Reshape, Flatten, Dropout, Activation\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential, load_model, Model\n",
    "import keras.models as models\n",
    "from keras import metrics\n",
    "from keras.regularizers import *\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers.noise import GaussianNoise\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer as LB\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open(\"RML2016.10b.dat\",'rb') \n",
    "dt = pickle.load(data, encoding = 'bytes') \n",
    "snrs, mods = map(lambda j: sorted(list(set(map(lambda x: x[j], dt.keys())))), [1,0])\n",
    "X = [] \n",
    "lbl = []\n",
    "for mod in mods:\n",
    "    for snr in snrs:\n",
    "        X.append(dt[(mod,snr)])\n",
    "        for i in range(dt[(mod,snr)].shape[0]):  lbl.append((mod,snr))\n",
    "X = np.vstack(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the data is split into training set, Validation set and test set\n",
    "#  randomly initializing the seed\n",
    "np.random.seed(2021)\n",
    "n_example = X.shape[0]\n",
    "# 80 % of input data was chosen for training\n",
    "n_train = n_example * 0.80\n",
    "# randomly choosing the input training signals\n",
    "train_idx = np.random.choice(range(0, n_example), size=int(n_train), replace=False)\n",
    "# test set\n",
    "test_idx = list(set(range(0, n_example)) - set(train_idx))\n",
    "X_train = X[train_idx]\n",
    "X_test = X[test_idx]\n",
    "\n",
    "# Function for Hot Encoding the vectors\n",
    "def onehot(vec):\n",
    "    vec_hot = np.zeros([len(vec), max(vec) + 1])\n",
    "    vec_hot[np.arange(len(vec)), vec] = 1\n",
    "    return vec_hot\n",
    "\n",
    "Y_train = onehot(list(map(lambda x: mods.index(lbl[x][0]), train_idx)))\n",
    "Y_test = onehot(list(map(lambda x: mods.index(lbl[x][0]), test_idx)))\n",
    "labels = np.array(lbl)\n",
    "snr_train = labels[train_idx][:,1].astype(int)\n",
    "snr_test = labels[test_idx][:,1].astype(int)\n",
    "in_shp = list(X_train.shape[1:])\n",
    "classes = mods\n",
    "print(X_train.shape,Y_train.shape, in_shp)\n",
    "print('classes:', mods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BASE CNN MODEL\n",
    "dr = 0.6\n",
    "model = models.Sequential()\n",
    "inp = Input(shape=(1,2, 128)) \n",
    "\n",
    "l1=(Conv2D(64, (1, 3), padding='same', activation=\"relu\", name=\"conv1\", kernel_initializer='glorot_uniform',\n",
    "                 data_format=\"channels_first\"))(inp)\n",
    "l1=(Dropout(dr))(l1)\n",
    "l2=(Conv2D(16, (2, 3), padding='same', activation=\"relu\", name=\"conv2\", kernel_initializer='glorot_uniform',\n",
    "                 data_format=\"channels_first\"))(l1)\n",
    "l2=(Dropout(dr))(l2)\n",
    "f = (Flatten())(l2)\n",
    "d1=(Dense(128, activation='relu',  name=\"dense1\"))(f)\n",
    "d2=(Dense(len(classes),  name=\"dense2\"))(d1)\n",
    "act=(Activation('softmax'))(d2)\n",
    "out=(Reshape([len(classes)]))(act)\n",
    "model = models.Model(inputs=inp, outputs=out)\n",
    "\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "X_train = np.reshape(X_train, (-1,1,2,128))\n",
    "X_test = np.reshape(X_test, (-1,1,2,128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history=model.fit(X_train, Y_train, epochs=100, validation_data=(X_test, Y_test), batch_size=1024, callbacks=[EarlyStopping(patience=35, restore_best_weights = True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"cnn.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "def plot_results(predicted_labels, true_labels, snrs):\n",
    "  sorted_snrs = np.sort(np.unique(snrs))\n",
    "  x_axis = []\n",
    "  y_axis = []\n",
    "  for snr in sorted_snrs:\n",
    "    idx = np.where(snrs == snr)\n",
    "    #print('snr =', snr, '-->', accuracy_score(np.argmax(true_labels[idx], axis = 1), np.argmax(predicted_labels[idx], axis = 1)))\n",
    "    x_axis.append(snr)\n",
    "    y_axis.append(accuracy_score(np.argmax(true_labels[idx], axis = 1), np.argmax(predicted_labels[idx], axis = 1)))\n",
    "  \n",
    "  plt.xlabel('SNR (in dB)')\n",
    "  plt.ylabel('Accuracy (%)')\n",
    "  plt.title('Classification Accuracy over different SNRs for CNN Model')\n",
    "  plt.plot(x_axis, np.array(y_axis) * 100, 'g*--')\n",
    "  plt.grid(True)\n",
    "\n",
    "def print_results(predicted_labels, true_labels, snrs):\n",
    "  sorted_snrs = np.sort(np.unique(snrs))\n",
    "  x_axis = []\n",
    "  y_axis = []\n",
    "  for snr in sorted_snrs:\n",
    "    idx = np.where(snrs == snr)\n",
    "    #print('snr =', snr, '-->', accuracy_score(np.argmax(true_labels[idx], axis = 1), np.argmax(predicted_labels[idx], axis = 1)))\n",
    "    x_axis.append(snr)\n",
    "    y_axis.append(accuracy_score(np.argmax(true_labels[idx], axis = 1), np.argmax(predicted_labels[idx], axis = 1)))\n",
    "  return df(data = np.array(y_axis).reshape(1, -1) * 100,  columns = sorted_snrs, index = ['Accuracy (%)']).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model = keras.models.load_model('cnn.h5')\n",
    "y_pred_cnn = cnn_model.predict(X_test)\n",
    "plot_results(y_pred_cnn, Y_test, snr_test)\n",
    "print_results(y_pred_cnn, Y_test, snr_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RUN THIS CELL RIGHT AFTER TRAINING THE MODEL AS IT WILL NOT BE STORED IN AN .h5 FILE.\n",
    "# Show loss curves\n",
    "plt.figure()\n",
    "plt.title('Training performance for CNN Model')\n",
    "plt.plot(history.epoch, history.history['loss'], label='train loss+error')\n",
    "plt.plot(history.epoch, history.history['val_loss'], label='val_error')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "y_pred=np.argmax(y_pred_cnn, axis=1)\n",
    "y_test=np.argmax(Y_test, axis=1)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "# Normalise\n",
    "cmn = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "sns.heatmap(cmn, annot=True, cmap='Greens', fmt='.2f', xticklabels=classes, yticklabels=classes)\n",
    "plt.title('Confusion Matrix for CNN Model')\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show(block=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import itertools\n",
    "\n",
    "test_Y_hat = y_pred_cnn #model.predict(X_test, batch_size=1024)\n",
    "conf = np.zeros([len(classes),len(classes)])\n",
    "confnorm = np.zeros([len(classes),len(classes)])\n",
    "for i in range(0,X_test.shape[0]):\n",
    "    j = list(Y_test[i,:]).index(1)\n",
    "    k = int(np.argmax(test_Y_hat[i,:]))\n",
    "    conf[j,k] = conf[j,k] + 1\n",
    "for i in range(0,len(classes)):\n",
    "    confnorm[i,:] = conf[i,:] / np.sum(conf[i,:])\n",
    "y_pred=np.argmax(y_pred_cnn, axis=1)\n",
    "y_test=np.argmax(Y_test, axis=1)\n",
    "cm = confusion_matrix(y_test,y_pred)\n",
    "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion Matrix for CNN Model without Normalization', cmap=plt.cm.Greens):\n",
    "    fig1 = plt.figure(figsize=(7, 7), dpi=65)\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion Matrix for CNN Model without Normalization')\n",
    "\n",
    "         \n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "            horizontalalignment=\"center\",\n",
    "            color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.title('Confusion Matrix for CNN Model without Normalization')\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "cm_plot_labels = ['8PSK', 'AM-DSB', 'BPSK', 'CPFSK', 'GFSK', 'PAM4', 'QAM16', 'QAM64', 'QPSK', 'WBFM']\n",
    "plot_confusion_matrix(cm=cm, classes=cm_plot_labels, title='Confusion Matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR ROC CURVE\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from scipy import interp\n",
    "from itertools import cycle\n",
    "lw=2\n",
    "n_classes=10\n",
    "\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(Y_test[:, i], y_pred_cnn[:, i])  #####\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(Y_test.ravel(), y_pred_cnn.ravel()) ####\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "\n",
    "# Then interpolate all ROC curves at this points\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for i in range(n_classes):\n",
    "    mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "# Finally average it and compute AUC\n",
    "mean_tpr /= n_classes\n",
    "\n",
    "fpr[\"macro\"] = all_fpr\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "# Plot all ROC curves\n",
    "plt.figure()\n",
    "plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "         label='micro-average ROC curve (area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"micro\"]),\n",
    "         color='deeppink', linestyle=':', linewidth=1)\n",
    "\n",
    "plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "         label='macro-average ROC curve (area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"macro\"]),\n",
    "         color='navy', linestyle=':', linewidth=1)\n",
    "\n",
    "colors = cycle(['brown', 'darkorange', 'cornflowerblue', 'black', 'green', 'red', 'purple', 'magenta', 'cyan','yellow'])\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
    "             label='ROC curve of class {0} (area = {1:0.2f})'\n",
    "             ''.format(i, roc_auc[i]))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "#plt.xlim([0.0, 1.0])\n",
    "#plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic (ROC) curve for CNN Model')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TO PLOT CONFUSION MATRIX FOR EVERY SINGLE SNR BUT WITHOUT VALUES IN IT.\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.Greens, labels=[]):\n",
    "    plt.figure()\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(labels))\n",
    "    plt.xticks(tick_marks, labels, rotation=45)\n",
    "    plt.yticks(tick_marks, labels)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    #plt.savefig(title)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Plot confusion matrix\n",
    "test_Y_hat = y_pred_cnn\n",
    "\n",
    "name = 'CNN Model'\n",
    "acc = {}\n",
    "for snr in snrs:\n",
    "\n",
    "    # extract classes @ SNR\n",
    "    test_SNRs = list(map(lambda x: lbl[x][1], test_idx))\n",
    "    test_X_i = X_test[np.where(np.array(test_SNRs) == snr)]\n",
    "    test_Y_i = Y_test[np.where(np.array(test_SNRs) == snr)]\n",
    "\n",
    "    # estimate classes\n",
    "    test_Y_i_hat = model.predict(test_X_i)\n",
    "\n",
    "\n",
    "    pre_labels_i = []\n",
    "    for x in test_Y_i_hat:\n",
    "        tmp = np.argmax(x, 0)\n",
    "        pre_labels_i.append(tmp)\n",
    "    true_labels_i = []\n",
    "    for x in test_Y_i:\n",
    "        tmp = np.argmax(x, 0)\n",
    "        true_labels_i.append(tmp)\n",
    "    cnf_matrix = confusion_matrix(true_labels_i, pre_labels_i)\n",
    "    # np.set_printoptions(precision=2)\n",
    "    # Plot non-normalized confusion matrix\n",
    "    # plt.figure()\n",
    "    plot_confusion_matrix(cnf_matrix, labels=classes,\n",
    "     #                            normalize=False,\n",
    "                                 title='%s Confusion matrix, without normalization (SNR=%d)' % (name, snr))\n",
    "    #plt.savefig('%s Confusion matrix, without normalization (SNR=%d)' % (name, snr))\n",
    "    # Plot normalized confusion matrix\n",
    "    # plt.figure()\n",
    "    plot_confusion_matrix(cnf_matrix, labels=classes,\n",
    "                       #          normalize=True,\n",
    "                                 title='%s Normalized confusion matrix (SNR=%d)' % (name, snr))\n",
    "   # plt.savefig('%s Normalized confusion matrix (SNR=%d)' % (name, snr))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    conf = np.zeros([len(classes), len(classes)])\n",
    "    confnorm = np.zeros([len(classes), len(classes)])\n",
    "    for i in range(0, test_X_i.shape[0]):\n",
    "        j = list(test_Y_i[i, :]).index(1)\n",
    "        k = int(np.argmax(test_Y_i_hat[i, :]))\n",
    "        conf[j, k] += 1\n",
    "    for i in range(0, len(classes)):\n",
    "        confnorm[i, :] = conf[i, :] / np.sum(conf[i, :])\n",
    "    # plt.figure()\n",
    "    plot_confusion_matrix(confnorm, labels=classes, title=\"%s Confusion Matrix (SNR=%d)\" % (name, snr))\n",
    "\n",
    "    cor = np.sum(np.diag(conf))\n",
    "    ncor = np.sum(conf) - cor\n",
    "    print (\"Overall Accuracy: \", cor / (cor + ncor))\n",
    "    acc[snr] = 1.0 * cor / (cor + ncor)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
